{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e817d3-fbc1-4ed8-a4b0-f61cfa9f66f2",
   "metadata": {},
   "source": [
    "# README.md\n",
    "## 测试机器\n",
    "Mac M2\n",
    "- vcpu 8\n",
    "- memory 16G\n",
    "## 安装程序\n",
    "- python 3.12\n",
    "- Bio 1.7.1\n",
    "- cutadapt 4.9\n",
    "- vsearch v2.30.0\n",
    "- FastQC v0.12.1\n",
    "- trimmomatic 0.39\n",
    "## 运行\n",
    "依次执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aee67b4-28e1-43ba-b2a6-df53aef64c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6ab8dd-d672-4901-8eba-320198f0f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_update_csv(csv_file_path, output_report_path=\"validation_report.txt\", delimiter=','):\n",
    "    \"\"\"\n",
    "    验证CSV文件中每行数据的序列位置关系，并在需要时更新end_base值，然后生成详细报告。\n",
    "\n",
    "    参数:\n",
    "    csv_file_path (str): CSV文件的路径\n",
    "    output_report_path (str): 输出报告文件的路径，默认为\"validation_report.txt\"\n",
    "    delimiter (str): CSV文件使用的分隔符，默认为逗号(',')\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含总体统计信息和详细结果的字典\n",
    "    \"\"\"\n",
    "    # 初始化结果统计\n",
    "    results = {\n",
    "        'total_rows': 0,\n",
    "        'passed_rows': 0,\n",
    "        'failed_rows': 0,\n",
    "        'details': [],\n",
    "        'errors': [],\n",
    "        'updates': [],  # 记录所有更新的行\n",
    "        'column_validation': {\n",
    "            'expected_columns': ['barcode1', 'barcode2', 'template', 'spacer', 'start_base', 'end_base', 'base_windows'],\n",
    "            'actual_columns': [],\n",
    "            'columns_match': False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 存储所有行数据以便后续可能的写回\n",
    "    all_rows = []\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            # 创建CSV字典读取器，第一行作为列名\n",
    "            reader = csv.DictReader(csvfile, delimiter=delimiter)\n",
    "            fieldnames = reader.fieldnames\n",
    "            \n",
    "            # 检查列名是否匹配\n",
    "            results['column_validation']['actual_columns'] = fieldnames\n",
    "            expected_set = set(results['column_validation']['expected_columns'])\n",
    "            actual_set = set(fieldnames) if fieldnames else set()\n",
    "            results['column_validation']['columns_match'] = (expected_set == actual_set)\n",
    "            \n",
    "            if not results['column_validation']['columns_match']:\n",
    "                missing_cols = expected_set - actual_set\n",
    "                extra_cols = actual_set - expected_set\n",
    "                error_msg = f\"列名不匹配! 缺失的列: {missing_cols}，多余的列: {extra_cols}\"\n",
    "                results['errors'].append(error_msg)\n",
    "                generate_report(results, output_report_path)\n",
    "                return results\n",
    "            \n",
    "            # 读取所有行数据\n",
    "            for row in reader:\n",
    "                all_rows.append(row)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        error_msg = f\"错误: 找不到文件 '{csv_file_path}'\"\n",
    "        results['errors'].append(error_msg)\n",
    "        print(error_msg)\n",
    "        generate_report(results, output_report_path)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        error_msg = f\"读取CSV文件时发生错误: {e}\"\n",
    "        results['errors'].append(error_msg)\n",
    "        print(error_msg)\n",
    "        generate_report(results, output_report_path)\n",
    "        return results\n",
    "    \n",
    "    # 处理每一行数据\n",
    "    needs_update = False\n",
    "    for row_index, row in enumerate(all_rows, start=1):\n",
    "        results['total_rows'] += 1\n",
    "        \n",
    "        # 初始化当前行的验证结果字典\n",
    "        current_result = {\n",
    "            'row_number': row_index + 1,  # +1 是因为从0开始计数，且跳过标题行\n",
    "            'barcode1': row['barcode1'],\n",
    "            'barcode2': row['barcode2'],\n",
    "            'template': row['template'],\n",
    "            'spacer': row['spacer'],\n",
    "            'end_base_original': row['end_base'],\n",
    "            'end_base_calculated': None,\n",
    "            'needs_update': False,\n",
    "            'checks': {\n",
    "                'barcode1_at_start': False,\n",
    "                'barcode2_at_end': False,\n",
    "                'spacer_in_template': False\n",
    "            },\n",
    "            'all_passed': False,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        # 获取当前行的各个字段值\n",
    "        barcode1 = row['barcode1']\n",
    "        barcode2 = row['barcode2']\n",
    "        template = row['template']\n",
    "        spacer = row['spacer']\n",
    "        \n",
    "        # 检验1: barcode1是否在template开头\n",
    "        if template.startswith(barcode1):\n",
    "            current_result['checks']['barcode1_at_start'] = True\n",
    "        else:\n",
    "            current_result['errors'].append(f\"barcode1 '{barcode1}' 不在template开头\")\n",
    "        \n",
    "        # 检验2: barcode2是否在template末尾\n",
    "        if template.endswith(barcode2):\n",
    "            current_result['checks']['barcode2_at_end'] = True\n",
    "        else:\n",
    "            current_result['errors'].append(f\"barcode2 '{barcode2}' 不在template末尾\")\n",
    "        \n",
    "        # 检验3: spacer是否在template中\n",
    "        if spacer in template:\n",
    "            current_result['checks']['spacer_in_template'] = True\n",
    "        else:\n",
    "            current_result['errors'].append(f\"spacer '{spacer}' 不在template中\")\n",
    "            results['details'].append(current_result)\n",
    "            results['failed_rows'] += 1\n",
    "            continue\n",
    "        \n",
    "        # 只有前3项检验都通过，才计算片段长度\n",
    "        if (current_result['checks']['barcode1_at_start'] and \n",
    "            current_result['checks']['barcode2_at_end'] and \n",
    "            current_result['checks']['spacer_in_template']):\n",
    "            \n",
    "            # 计算从spacer开始(包含)到barcode2开始之前的片段长度\n",
    "            spacer_start = template.find(spacer)\n",
    "            barcode2_start = template.find(barcode2)\n",
    "            \n",
    "            # 提取从spacer开始到barcode2开始之前的片段\n",
    "            fragment = template[spacer_start:barcode2_start]\n",
    "            fragment_length = len(fragment)\n",
    "            current_result['end_base_calculated'] = fragment_length\n",
    "            \n",
    "            # 检查是否需要更新end_base\n",
    "            try:\n",
    "                end_base_original = int(row['end_base'])\n",
    "                if fragment_length != end_base_original:\n",
    "                    current_result['needs_update'] = True\n",
    "                    row['end_base'] = str(fragment_length)  # 更新内存中的值\n",
    "                    needs_update = True\n",
    "                    results['updates'].append({\n",
    "                        'row': row_index + 1,\n",
    "                        'original': end_base_original,\n",
    "                        'calculated': fragment_length\n",
    "                    })\n",
    "            except ValueError:\n",
    "                current_result['errors'].append(f\"end_base值 '{row['end_base']}' 不是有效的整数\")\n",
    "            \n",
    "            # 当前3项检验通过时，就算通过\n",
    "            current_result['all_passed'] = True\n",
    "            results['passed_rows'] += 1\n",
    "        else:\n",
    "            results['failed_rows'] += 1\n",
    "        \n",
    "        results['details'].append(current_result)\n",
    "    \n",
    "    # 如果需要更新，写回CSV文件\n",
    "    if needs_update:\n",
    "        try:\n",
    "            with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=delimiter)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_rows)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"更新CSV文件时发生错误: {e}\"\n",
    "            results['errors'].append(error_msg)\n",
    "            print(error_msg)\n",
    "    \n",
    "    # 生成报告文件\n",
    "    generate_report(results, output_report_path)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"检验完成: 共{results['total_rows']}行, \"\n",
    "          f\"通过{results['passed_rows']}行, \"\n",
    "          f\"失败{results['failed_rows']}行\")\n",
    "    \n",
    "    if results.get('updates'):\n",
    "        print(f\"更新了 {len(results['updates'])} 行的 end_base 值\")\n",
    "    \n",
    "    if results['failed_rows'] > 0:\n",
    "        raise Exception(f\"检验未全部通过，存在 {results['failed_rows']} 个错误。请先处理错误再执行后续操作。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c652c8-2c65-4783-b790-ae05c04c7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(results, output_path):\n",
    "    \"\"\"\n",
    "    生成详细的检验报告\n",
    "    \n",
    "    参数:\n",
    "    results (dict): 检验结果字典\n",
    "    output_path (str): 输出报告文件的路径\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as report_file:\n",
    "        # 写入报告头部\n",
    "        report_file.write(\"=\" * 80 + \"\\n\")\n",
    "        report_file.write(\"CSV文件序列检验与更新报告\\n\")\n",
    "        report_file.write(f\"生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        report_file.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # 写入列名验证结果\n",
    "        report_file.write(\"列名验证:\\n\")\n",
    "        report_file.write(f\"期望的列名: {results['column_validation']['expected_columns']}\\n\")\n",
    "        report_file.write(f\"实际的列名: {results['column_validation']['actual_columns']}\\n\")\n",
    "        report_file.write(f\"列名是否匹配: {'是' if results['column_validation']['columns_match'] else '否'}\\n\")\n",
    "        \n",
    "        if not results['column_validation']['columns_match']:\n",
    "            missing_cols = set(results['column_validation']['expected_columns']) - set(results['column_validation']['actual_columns'])\n",
    "            extra_cols = set(results['column_validation']['actual_columns']) - set(results['column_validation']['expected_columns'])\n",
    "            report_file.write(f\"缺失的列: {missing_cols}\\n\")\n",
    "            report_file.write(f\"多余的列: {extra_cols}\\n\")\n",
    "        report_file.write(\"\\n\")\n",
    "        \n",
    "        # 写入统计信息\n",
    "        report_file.write(\"总体统计:\\n\")\n",
    "        report_file.write(f\"总行数: {results['total_rows']}\\n\")\n",
    "        report_file.write(f\"通过行数: {results['passed_rows']}\\n\")\n",
    "        report_file.write(f\"失败行数: {results['failed_rows']}\\n\")\n",
    "        if results['total_rows'] > 0:\n",
    "            report_file.write(f\"通过率: {results['passed_rows']/results['total_rows']*100:.2f}%\\n\")\n",
    "        \n",
    "        # 写入更新统计\n",
    "        if results.get('updates'):\n",
    "            report_file.write(f\"更新行数: {len(results['updates'])}\\n\")\n",
    "        report_file.write(\"\\n\")\n",
    "        \n",
    "        # 如果有错误信息，写入错误\n",
    "        if results.get('errors'):\n",
    "            report_file.write(\"全局错误:\\n\")\n",
    "            for error in results['errors']:\n",
    "                report_file.write(f\"  - {error}\\n\")\n",
    "            report_file.write(\"\\n\")\n",
    "        \n",
    "        # 写入更新详情\n",
    "        if results.get('updates'):\n",
    "            report_file.write(\"end_base更新详情:\\n\")\n",
    "            for update in results['updates']:\n",
    "                report_file.write(f\"  行 {update['row']}: {update['original']} → {update['calculated']}\\n\")\n",
    "            report_file.write(\"\\n\")\n",
    "        \n",
    "        # 写入每行的详细结果\n",
    "        if results['details']:\n",
    "            report_file.write(\"详细检验结果:\\n\")\n",
    "            report_file.write(\"-\" * 80 + \"\\n\")\n",
    "            \n",
    "            for detail in results['details']:\n",
    "                report_file.write(f\"行号: {detail['row_number']}\\n\")\n",
    "                report_file.write(f\"barcode1: {detail['barcode1']}\\n\")\n",
    "                report_file.write(f\"barcode2: {detail['barcode2']}\\n\")\n",
    "                report_file.write(f\"spacer: {detail['spacer']}\\n\")\n",
    "                report_file.write(f\"end_base(原始): {detail['end_base_original']}\\n\")\n",
    "                \n",
    "                if detail['end_base_calculated'] is not None:\n",
    "                    report_file.write(f\"end_base(计算): {detail['end_base_calculated']}\\n\")\n",
    "                \n",
    "                if detail['needs_update']:\n",
    "                    report_file.write(\"end_base状态: 已更新 ✓\\n\")\n",
    "                \n",
    "                # 写入检验结果\n",
    "                report_file.write(\"检验结果: \")\n",
    "                if detail['all_passed']:\n",
    "                    report_file.write(\"所有检验通过 ✓\\n\")\n",
    "                else:\n",
    "                    report_file.write(\"存在错误 ✗\\n\")\n",
    "                    \n",
    "                # 写入具体检验项结果\n",
    "                report_file.write(\"  - barcode1在开头: \")\n",
    "                report_file.write(\"通过 ✓\\n\" if detail['checks']['barcode1_at_start'] else \"失败 ✗\\n\")\n",
    "                \n",
    "                report_file.write(\"  - barcode2在末尾: \")\n",
    "                report_file.write(\"通过 ✓\\n\" if detail['checks']['barcode2_at_end'] else \"失败 ✗\\n\")\n",
    "                \n",
    "                report_file.write(\"  - spacer在template中: \")\n",
    "                report_file.write(\"通过 ✓\\n\" if detail['checks']['spacer_in_template'] else \"失败 ✗\\n\")\n",
    "                \n",
    "                # 写入错误信息（如果有）\n",
    "                if detail['errors']:\n",
    "                    report_file.write(\"错误信息:\\n\")\n",
    "                    for error in detail['errors']:\n",
    "                        report_file.write(f\"  - {error}\\n\")\n",
    "                \n",
    "                report_file.write(\"-\" * 80 + \"\\n\")\n",
    "        \n",
    "        # 写入报告尾部\n",
    "        report_file.write(\"\\n报告结束\\n\")\n",
    "        report_file.write(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(f\"检验报告已生成: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c71904b-b355-40c5-bcfa-774b2d226ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_QC(input_r1, input_r2, output_dir, primer_f=\"\", primer_r=\"\"):\n",
    "    \"\"\"\n",
    "    执行严格的NGS数据质控流程\n",
    "    :param input_r1: Read1输入文件路径\n",
    "    :param input_r2: Read2输入文件路径\n",
    "    :param output_dir: 输出目录\n",
    "    :param primer_f: 正向引物序列（可选）\n",
    "    :param primer_r: 反向引物序列（可选）\n",
    "    \"\"\"\n",
    "    # 定义接头序列\n",
    "    adapter_r1 = \"AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\"  # P7 adapter for read1\n",
    "    adapter_r2 = \"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"  # P5 adapter for read2\n",
    "\n",
    "    subprocess.run([\"mkdir\", \"-p\", f\"{output_dir}/fastqc_raw\"])\n",
    "    subprocess.run([\"mkdir\", \"-p\", f\"{output_dir}/fastqc_trimmed\"])\n",
    "    \n",
    "    # FastQC原始数据质控\n",
    "    subprocess.run([\"fastqc\", \"-t\", \"14\", input_r1, input_r2, \"-o\", f\"{output_dir}/fastqc_raw\"])\n",
    "    \n",
    "    # cutadapt切除接头与引物\n",
    "    cutadapt_cmd = [\n",
    "            \"cutadapt\",\n",
    "            \"-a\", adapter_r2,  # R2的3'端接头（P5）\n",
    "            \"-A\", adapter_r1,  # R1的3'端接头（P7）\n",
    "            \"-o\", f\"{output_dir}/F.fq.gz\",\n",
    "            \"-p\", f\"{output_dir}/R.fq.gz\",\n",
    "            \"--minimum-length\", \"50\",\n",
    "            \"--max-n\", \"0\",\n",
    "            \"--error-rate\", \"0.1\",\n",
    "            f\"--json={output_dir}/cutadapt.json\",\n",
    "            \"--cores=14\"\n",
    "        ]\n",
    "        \n",
    "    # 添加引物切除参数\n",
    "    if primer_f and primer_r:\n",
    "        cutadapt_cmd.extend([\"-g\", f\"^{primer_f}\", \"-G\", f\"^{primer_r}\"])\n",
    "    \n",
    "    cutadapt_cmd.extend([input_r1, input_r2])\n",
    "    \n",
    "    subprocess.run(cutadapt_cmd)\n",
    "    \n",
    "    # 切除后质控验证\n",
    "    try:\n",
    "        with open(f\"{output_dir}/cutadapt.json\") as f:\n",
    "            log_data = json.load(f)\n",
    "            \n",
    "        total_pairs = log_data[\"read_counts\"][\"input\"]\n",
    "        kept_pairs = log_data[\"read_counts\"][\"output\"]\n",
    "        kept_ratio = kept_pairs / total_pairs * 100\n",
    "        \n",
    "        print(f\"原始序列对: {total_pairs}\")\n",
    "        print(f\"保留序列对: {kept_pairs} ({kept_ratio:.2f}%)\")\n",
    "        \n",
    "        # 验证标准\n",
    "        if kept_ratio < 90:\n",
    "            print(\"\\n⚠️ 警告: 保留率低于90%，建议检查接头/引物设计\")\n",
    "        else:\n",
    "            print(\"\\n✅ 保留率符合质控标准(>90%)\")\n",
    "                \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON解析失败: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ cutadapt.json文件未生成，请检查命令执行\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ JSON结构异常，缺失关键字段: {e}\")\n",
    "    \n",
    "    # 额外质控：切除后FastQC验证\n",
    "    subprocess.run([\n",
    "        \"fastqc\", \n",
    "        \"-t\", \"14\",\n",
    "        f\"{output_dir}/F.fq.gz\", \n",
    "        f\"{output_dir}/R.fq.gz\",\n",
    "        \"-o\", f\"{output_dir}/fastqc_trimmed\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09dbce8-73e3-4e9a-8680-0055fd091da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merget(input_forward, input_reverse, output_merged):\n",
    "    \"\"\"合并双端测序数据，若输出文件已存在则跳过\"\"\"\n",
    "    # 检查输出文件是否已存在\n",
    "    if os.path.exists(output_merged):\n",
    "        print(f\"文件 {output_merged} 已存在，跳过合并操作\")\n",
    "        return\n",
    "    \n",
    "    # 检查输入文件是否存在\n",
    "    if not os.path.exists(input_forward):\n",
    "        raise FileNotFoundError(f\"正向文件不存在: {input_forward}\")\n",
    "    if not os.path.exists(input_reverse):\n",
    "        raise FileNotFoundError(f\"反向文件不存在: {input_reverse}\")\n",
    "    \n",
    "    # 构建并执行VSEARCH命令\n",
    "    vsearch_command = (\n",
    "        f\"vsearch --fastq_mergepairs {input_forward} \"\n",
    "        f\"--reverse {input_reverse} \"\n",
    "        f\"--fastqout {output_merged} \"\n",
    "        \"--fastq_allowmergestagger\"\n",
    "    )\n",
    "    subprocess.run(vsearch_command, shell=True, check=True)\n",
    "    print(f\"双端合并完成 → {output_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4997126-f950-4cb1-bb77-8943abf7d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC_merger(input_fastq, output_dir, output_fastq):\n",
    "    # 检查输出文件是否已存在（若存在则跳过）\n",
    "    if os.path.exists(output_fastq):\n",
    "        print(f\"文件 {output_fastq} 已存在，跳过质量控制和修剪步骤\")\n",
    "        return\n",
    "    \n",
    "    # 创建输出目录（若不存在）\n",
    "    subprocess.run([\"mkdir\", \"-p\", output_dir])\n",
    "    \n",
    "    # 执行FastQC质量控制\n",
    "    subprocess.run([\n",
    "        \"fastqc\", \n",
    "        \"-t\", \"14\", \n",
    "        input_fastq, \n",
    "        \"-o\", output_dir\n",
    "    ])\n",
    "    \n",
    "    # 执行Trimmomatic修剪\n",
    "    subprocess.run([\n",
    "        \"trimmomatic\", \"SE\", \n",
    "        \"-threads\", \"14\", \n",
    "        \"-phred33\", \n",
    "        input_fastq, \n",
    "        output_fastq, \n",
    "        \"LEADING:3\", \n",
    "        \"TRAILING:3\", \n",
    "        \"SLIDINGWINDOW:4:15\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9126988-3a6b-4c2f-9391-7cb343d1e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarcodeClassifier:\n",
    "    def __init__(self, input_fastq, csv_file, output_directory):\n",
    "        self.input_fastq = input_fastq\n",
    "        self.csv_file = csv_file\n",
    "        self.output_directory = output_directory\n",
    "        self.barcode_pairs = self.read_barcodes_from_csv()\n",
    "        self.barcode_handles = {}\n",
    "        # 创建输出目录（如果不存在）\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    def read_barcodes_from_csv(self):\n",
    "        \"\"\"从CSV文件中读取条形码组合\"\"\"\n",
    "        barcode_pairs = []\n",
    "        with open(self.csv_file, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            next(csvreader)  # 跳过标题行\n",
    "            for row in csvreader:\n",
    "                barcode1, barcode2 = row[0].strip(), row[1].strip()\n",
    "                barcode_pairs.append((barcode1, barcode2))\n",
    "        return barcode_pairs\n",
    "\n",
    "    def correct_sequence(self, sequence):\n",
    "        \"\"\"校正反向互补序列\"\"\"\n",
    "        base_F = \"ATCG\"\n",
    "        base_R = \"TAGC\"\n",
    "        complement = {f: r for f, r in zip(base_F, base_R)}\n",
    "        return ''.join(complement.get(base, base) for base in reversed(sequence))\n",
    "\n",
    "    def output_files_exist(self, missing_threshold_num = 0.5):\n",
    "        \"\"\"检查所有输出文件是否已存在，并生成缺失文件报告\"\"\"\n",
    "        report_lines = []  # 用于收集报告内容\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        report_lines.append(f\"文件存在性检查报告生成时间: {current_time}\")\n",
    "        report_lines.append(f\"检查目录: {self.output_directory}\")\n",
    "        report_lines.append(\"-\" * 50)\n",
    "    \n",
    "        # 检查未匹配序列文件\n",
    "        unmatched_file = os.path.join(self.output_directory, \"unmatched_output.fastq\")\n",
    "        if not os.path.exists(unmatched_file):\n",
    "            report_lines.append(\"未匹配序列文件 'unmatched_output.fastq' 不存在。\")\n",
    "            self._generate_report(report_lines)  # 即使未匹配文件缺失也生成报告\n",
    "            return False\n",
    "    \n",
    "        total_barcode_pairs = len(self.barcode_pairs)\n",
    "        missing_threshold = missing_threshold_num * total_barcode_pairs  # 阈值\n",
    "        missing_files = []  # 用于存储缺失的条形码对\n",
    "    \n",
    "        # 检查每个条形码的输出文件\n",
    "        for barcode1, barcode2 in self.barcode_pairs:\n",
    "            barcode_file = os.path.join(\n",
    "                self.output_directory, \n",
    "                f\"{barcode1}_{barcode2}_output.fastq\"\n",
    "            )\n",
    "            if not os.path.exists(barcode_file):\n",
    "                missing_files.append((barcode1, barcode2))\n",
    "                report_lines.append(f\"条形码文件 '{barcode1}_{barcode2}_output.fastq' 不存在。\")\n",
    "    \n",
    "        # 计算缺失数量\n",
    "        missing_count = len(missing_files)\n",
    "        report_lines.append(\"-\" * 50)\n",
    "        report_lines.append(f\"条形码对总数: {total_barcode_pairs}\")\n",
    "        report_lines.append(f\"缺失文件数: {missing_count}\")\n",
    "        report_lines.append(f\"缺失阈值 ({missing_threshold_num*100}%): {missing_threshold:.1f}\")\n",
    "    \n",
    "        # 如果缺失数量超过阈值\n",
    "        if missing_count > missing_threshold:\n",
    "            report_lines.append(f\"结论: 缺失数量超过阈值，建议检查。\")\n",
    "            self._generate_report(report_lines)  # 生成报告\n",
    "            print(f\"缺失的条形码文件数量 ({missing_count}) 超过总条形码对数量的10% ({missing_threshold:.1f})。\")\n",
    "            for barcode1, barcode2 in missing_files:\n",
    "                print(f\"缺失的条形码对: {barcode1} -- {barcode2}\")\n",
    "            return False\n",
    "        else:\n",
    "            # 如果缺失率不超过10%，则认为文件存在（即使有少量缺失）\n",
    "            if missing_count > 0:\n",
    "                report_lines.append(f\"结论: 有{missing_count}个文件缺失，但未超过阈值，已忽略。缺失原因可能是原始序列中不存在能匹配 barcode1 开头且 barcode2 结尾的序列。\")\n",
    "            else:\n",
    "                report_lines.append(\"结论: 所有文件均存在。\")\n",
    "            self._generate_report(report_lines)  # 生成报告\n",
    "            if missing_count > 0:\n",
    "                print(f\"有{missing_count}个条形码文件缺失，但未超过阈值 ({missing_threshold:.1f})，忽略缺失。\")\n",
    "            return True\n",
    "    \n",
    "    def _generate_report(self, report_content):\n",
    "        \"\"\"生成并保存报告文件\"\"\"\n",
    "        report_filename = os.path.join(self.output_directory, \"file_validation_report.txt\")\n",
    "        try:\n",
    "            with open(report_filename, 'w', encoding='utf-8') as report_file: \n",
    "                report_file.write(\"\\n\".join(report_content))\n",
    "            print(f\"详细报告已保存至: {report_filename}\")\n",
    "        except IOError as e:\n",
    "            print(f\"写入报告文件时出错: {e}\")\n",
    "\n",
    "    def classify_by_barcodes(self):\n",
    "        \"\"\"执行条形码分类（如果输出文件不存在）\"\"\"\n",
    "        # 检查所有输出文件是否已存在\n",
    "        if self.output_files_exist():\n",
    "            print(\"输出文件未超过缺失阈值，跳过分类操作。\")\n",
    "            return\n",
    "            \n",
    "        print(\"开始处理序列...\")\n",
    "        # 打开未匹配序列文件\n",
    "        unmatched_handle = open(os.path.join(self.output_directory, \"unmatched_output.fastq\"), \"w\")\n",
    "        \n",
    "        with open(self.input_fastq, \"r\") as handle:\n",
    "            for record in tqdm.tqdm(SeqIO.parse(handle, \"fastq\"), desc=\"Processing sequences\"):\n",
    "                found_match = False\n",
    "                \n",
    "                # 正向匹配\n",
    "                for barcode1, barcode2 in self.barcode_pairs:\n",
    "                    if str(record.seq).startswith(barcode1) and str(record.seq).endswith(barcode2):\n",
    "                        self._write_record(record, barcode1, barcode2)\n",
    "                        found_match = True\n",
    "                        break\n",
    "                \n",
    "                # 反向互补匹配\n",
    "                if not found_match:\n",
    "                    corrected_seq = self.correct_sequence(str(record.seq))\n",
    "                    for barcode1, barcode2 in self.barcode_pairs:\n",
    "                        if corrected_seq.startswith(barcode1) and corrected_seq.endswith(barcode2):\n",
    "                            self._write_corrected_record(record, corrected_seq, barcode1, barcode2)\n",
    "                            found_match = True\n",
    "                            break\n",
    "                \n",
    "                # 未匹配序列\n",
    "                if not found_match:\n",
    "                    SeqIO.write(record, unmatched_handle, \"fastq\")\n",
    "\n",
    "        # 关闭所有文件句柄\n",
    "        for handle in self.barcode_handles.values():\n",
    "            handle.close()\n",
    "        unmatched_handle.close()\n",
    "    \n",
    "    def _write_record(self, record, barcode1, barcode2):\n",
    "        \"\"\"写入匹配的序列记录\"\"\"\n",
    "        barcode_pair_name = f\"{barcode1}_{barcode2}\"\n",
    "        if barcode_pair_name not in self.barcode_handles:\n",
    "            file_path = os.path.join(self.output_directory, f\"{barcode_pair_name}_output.fastq\")\n",
    "            self.barcode_handles[barcode_pair_name] = open(file_path, \"w\")\n",
    "        SeqIO.write(record, self.barcode_handles[barcode_pair_name], \"fastq\")\n",
    "    \n",
    "    def _write_corrected_record(self, record, corrected_seq, barcode1, barcode2):\n",
    "        \"\"\"写入校正后的序列记录\"\"\"\n",
    "        barcode_pair_name = f\"{barcode1}_{barcode2}\"\n",
    "        if barcode_pair_name not in self.barcode_handles:\n",
    "            file_path = os.path.join(self.output_directory, f\"{barcode_pair_name}_output.fastq\")\n",
    "            self.barcode_handles[barcode_pair_name] = open(file_path, \"w\")\n",
    "        \n",
    "        # 创建校正后的记录（保留质量值）\n",
    "        corrected_record = SeqRecord(\n",
    "            Seq(corrected_seq),\n",
    "            id=record.id,\n",
    "            description=record.description,\n",
    "            letter_annotations={\"phred_quality\": record.letter_annotations['phred_quality']}\n",
    "        )\n",
    "        SeqIO.write(corrected_record, self.barcode_handles[barcode_pair_name], \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd3e904-21e0-4ac7-9afb-1dc2bbeade7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(config_file):\n",
    "    with open(config_file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # 跳过标题\n",
    "        for row in tqdm.tqdm(csvreader):\n",
    "            barcode1 = row[0].strip()\n",
    "            barcode2 = row[1].strip()\n",
    "            template = row[2].strip().upper()\n",
    "            spacer = row[3].strip().upper()\n",
    "            start_base = int(row[4].strip())\n",
    "            end_base = int(row[5].strip())\n",
    "            ind = template.index(spacer)\n",
    "            sequences = [] \n",
    "            file_path = os.path.join(file, f\"{barcode1}_{barcode2}_output.fastq\")\n",
    "            try:\n",
    "                with open(file_path, \"r\") as seq_file:\n",
    "                    line_number = 0\n",
    "                    for line in seq_file:\n",
    "                        line_number += 1\n",
    "                        if line_number % 4 == 2:\n",
    "                            sequence = line.strip()\n",
    "                            extracted_sequence = sequence[ind+start_base:ind+end_base]\n",
    "                            sequences.append(extracted_sequence)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    \n",
    "            sequence_counts = Counter(sequences)\n",
    "\n",
    "            os.makedirs(os.path.join(file, \"ExtractSeq\"), exist_ok=True)\n",
    "            output_file_path = os.path.join(file, \"ExtractSeq\", os.path.basename(file_path).replace('.fastq', '_counts.csv'))\n",
    "            with open(output_file_path, 'w', newline='') as output_csvfile:\n",
    "                writer = csv.writer(output_csvfile)\n",
    "                writer.writerow(['Extracted Sequence', 'Count'])  \n",
    "                for seq, count in sequence_counts.items():\n",
    "                    writer.writerow([seq, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2afa1bc8-dd47-4572-ab02-d4f2d00e147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_base_counts(config_file, output_file):\n",
    "    results = []\n",
    "    with open(config_file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # 跳过标题\n",
    "        for row in tqdm.tqdm(csvreader):\n",
    "            barcode1 = row[0].strip()\n",
    "            barcode2 = row[1].strip()\n",
    "            template = row[2].strip().upper()\n",
    "            spacer = row[3].strip().upper()\n",
    "            base_windows = int(row[6].strip())\n",
    "            file_path = os.path.join(file, f\"{barcode1}_{barcode2}_output.fastq\")\n",
    "            sequences = []\n",
    "            try:\n",
    "                with open(file_path, 'r') as countsfile:\n",
    "                    line_number = 0\n",
    "                    for line in countsfile:\n",
    "                        line_number += 1\n",
    "                        if line_number % 4 == 2:\n",
    "                            sequences.append(line.strip())\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            ind = template.index(spacer)\n",
    "            base_windows_ind = ind + base_windows - 1\n",
    "\n",
    "            num_A = 0\n",
    "            num_T = 0\n",
    "            num_C = 0\n",
    "            num_G = 0\n",
    "\n",
    "            for seq in sequences:\n",
    "                if len(seq) > base_windows_ind:\n",
    "                    if seq[base_windows_ind] == 'A':\n",
    "                        num_A += 1\n",
    "                    elif seq[base_windows_ind] == 'T':\n",
    "                        num_T += 1\n",
    "                    elif seq[base_windows_ind] == 'C':\n",
    "                        num_C += 1\n",
    "                    elif seq[base_windows_ind] == 'G':\n",
    "                        num_G += 1\n",
    "\n",
    "            results.append([file_path, spacer, base_windows, num_A, num_T, num_C, num_G])\n",
    "\n",
    "    # 写入结果到CSV文件\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([ 'Spacer', 'Base Windows', 'A', 'T', 'C', 'G'])\n",
    "        writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fdd2fef-263f-4ccb-b0a6-e02be5e44443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_sequences(config_file):\n",
    "    \"\"\"处理序列数据，保存spacer定位后的完整序列\"\"\"\n",
    "    with open(config_file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # 跳过标题行\n",
    "        for row in tqdm.tqdm(csvreader):\n",
    "            # 解析配置参数\n",
    "            barcode1 = row[0].strip()\n",
    "            barcode2 = row[1].strip()\n",
    "            template = row[2].strip().upper()\n",
    "            spacer = row[3].strip().upper()\n",
    "            \n",
    "            # 定位spacer位置（用于验证序列有效性）\n",
    "            try:\n",
    "                ind = template.index(spacer)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # 构建输入文件路径\n",
    "            file_path = os.path.join(file,f\"{barcode1}_{barcode2}_output.fastq\")\n",
    "            \n",
    "            # 读取并保存完整序列\n",
    "            sequences = []\n",
    "            try:\n",
    "                with open(file_path, \"r\") as seq_file:\n",
    "                    for line_num, line in enumerate(seq_file, 1):\n",
    "                        if line_num % 4 == 2:  # 序列行\n",
    "                            sequences.append(line.strip())\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "                \n",
    "            # 统计序列频率\n",
    "            sequence_counts = Counter(sequences)\n",
    "            \n",
    "            # 输出结果到CSV\n",
    "            os.makedirs(os.path.join(file, \"AllSeq\"), exist_ok=True)\n",
    "            output_file_path = os.path.join(file, \"AllSeq\", os.path.basename(file_path).replace('.fastq', '_counts.csv'))\n",
    "            with open(output_file_path, 'w', newline='') as output_csvfile:\n",
    "                writer = csv.writer(output_csvfile)\n",
    "                writer.writerow(['Full Sequence', 'Count'])\n",
    "                for seq, count in sequence_counts.items():\n",
    "                    writer.writerow([seq, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac02d267-c9d4-4333-9146-d2ea2e96c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_surrounding_sequences(config_file, spacer_front=20, spacer_after=20):\n",
    "    \"\"\"处理序列数据，保存spacer定位区域前后特定bp的序列\"\"\"\n",
    "    with open(config_file, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # 跳过标题行\n",
    "        for row in tqdm.tqdm(csvreader, desc=\"Processing spacer surrounding sequences\"):\n",
    "            # 解析配置参数\n",
    "            barcode1 = row[0].strip()\n",
    "            barcode2 = row[1].strip()\n",
    "            template = row[2].strip().upper()\n",
    "            spacer = row[3].strip().upper()\n",
    "            \n",
    "            # 定位spacer位置\n",
    "            try:\n",
    "                ind = template.index(spacer)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # 构建输入文件路径\n",
    "            file_path = os.path.join(file, f\"{barcode1}_{barcode2}_output.fastq\")\n",
    "            \n",
    "            # 读取并处理序列\n",
    "            sequences = []\n",
    "            try:\n",
    "                with open(file_path, \"r\") as seq_file:\n",
    "                    for line_num, line in enumerate(seq_file, 1):\n",
    "                        if line_num % 4 == 2:  # 序列行\n",
    "                            sequence = line.strip()\n",
    "                            # 计算截取范围（含边界保护）\n",
    "                            start_index = max(0, ind - spacer_front)\n",
    "                            end_index = min(len(sequence), ind + len(spacer) + spacer_after)\n",
    "                            extracted_sequence = sequence[start_index:end_index]\n",
    "                            sequences.append(extracted_sequence)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "                \n",
    "            # 统计序列频率\n",
    "            sequence_counts = Counter(sequences)\n",
    "            \n",
    "            # 输出结果到CSV\n",
    "            os.makedirs(os.path.join(file, \"SurroundingSeq\"), exist_ok=True)\n",
    "            output_file_path = os.path.join(\n",
    "                file, \"SurroundingSeq\", \n",
    "                os.path.basename(file_path).replace('.fastq', '_counts.csv')\n",
    "            )\n",
    "            with open(output_file_path, 'w', newline='') as output_csvfile:\n",
    "                writer = csv.writer(output_csvfile)\n",
    "                writer.writerow(['Surrounding Sequence', 'Count'])\n",
    "                for seq, count in sequence_counts.items():\n",
    "                    writer.writerow([seq, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7926dc9f-4744-4c13-aa39-1bb5d43e5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGS(file,purpose,tempalte_CSV,spacer_front=20, spacer_after=20):    \n",
    "    input_forward  = file + \"/F.fq.gz\"\n",
    "    input_reverse = file + \"/R.fq.gz\"\n",
    "    output_merged = file + \"/merged.fasta\"\n",
    "    merget(input_forward,input_reverse,output_merged)\n",
    "    input_fastq = output_merged\n",
    "    output_dir = file + \"/fastqc_output\"\n",
    "    output_fastq = file + \"/output_trimmed.fastq\"\n",
    "    QC_merger(input_fastq,output_dir,output_fastq)\n",
    "    input_fastq = output_fastq\n",
    "    output_directory = file\n",
    "    classifier = BarcodeClassifier(input_fastq, tempalte_CSV, output_directory)\n",
    "    classifier.classify_by_barcodes()\n",
    "    if purpose == 1:\n",
    "        config_file = tempalte_CSV\n",
    "        process_sequences(config_file)\n",
    "    elif purpose == 2:\n",
    "        config_file = tempalte_CSV\n",
    "        output_file = file+ \"/results.csv\"\n",
    "        process_base_counts(config_file, output_file)\n",
    "    elif purpose == 3:\n",
    "        config_file = tempalte_CSV\n",
    "        process_all_sequences(config_file)\n",
    "    elif purpose == 4:\n",
    "        config_file = tempalte_CSV\n",
    "        process_surrounding_sequences(config_file, spacer_front=20, spacer_after=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111df4da-847e-49df-bc8d-5e345a430f3e",
   "metadata": {},
   "source": [
    "上述程序直接点击运行即可，不要进行任何修改，如果想修改的话请复制后进行修改\n",
    "二代测序数据下载后请自行解压，并且把文件名修改为F.fq和R.fq\n",
    "上传后确定文件夹后，在file中输入上传的路径，注意所有的后续文件都会有在这个文件夹中生成，建议每次都新建一个文件夹\n",
    "purpose只有三个选项有意义，1,2,3或者4，1是针对spacer定位的区域进行扫描并保存截取部分，2是针对碱基编辑器，3是针对spacer定位的区域进行扫描并将全部序列保存，4是针对spacer定位的区域前后特定bp的序列进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b93eac-715c-4890-97c6-faf6042522cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./\" # 生成文件的目录\n",
    "tempalte_CSV = \"tempalte_CSV.csv\" # tempalte_CSV文件目录\n",
    "origin_F = './1-LHH23167_L1_1.fq' # 正向原始文件目录\n",
    "origin_R = './1-LHH23167_L1_2.fq' # 反向原始文件目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df39eeda-198d-4e6c-a5a8-926ecf2c9a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检验报告已生成: validation_report.txt\n",
      "检验完成: 共36行, 通过36行, 失败0行\n"
     ]
    }
   ],
   "source": [
    "validate_and_update_csv(tempalte_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf53737-af4d-4a15-a28e-f3d9b448d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n",
      "null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started analysis of 1-LHH23167_L1_1.fq\n",
      "Started analysis of 1-LHH23167_L1_2.fq\n",
      "Approx 5% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 5% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 10% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 10% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 15% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 15% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 20% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 20% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 25% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 25% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 30% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 30% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 35% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 35% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 40% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 40% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 45% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 45% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 50% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 50% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 55% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 55% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 60% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 60% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 65% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 65% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 70% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 70% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 75% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 75% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 80% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 80% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 85% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 85% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 90% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 90% complete for 1-LHH23167_L1_2.fq\n",
      "Approx 95% complete for 1-LHH23167_L1_1.fq\n",
      "Approx 95% complete for 1-LHH23167_L1_2.fq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete for 1-LHH23167_L1_1.fq\n",
      "Analysis complete for 1-LHH23167_L1_2.fq\n",
      "This is cutadapt 5.1 with Python 3.12.11\n",
      "Command line parameters: -a AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT -A AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC -o .//F.fq.gz -p .//R.fq.gz --minimum-length 50 --max-n 0 --error-rate 0.1 --json=.//cutadapt.json --cores=14 ./1-LHH23167_L1_1.fq ./1-LHH23167_L1_2.fq\n",
      "Processing paired-end reads on 14 cores ...\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "Total read pairs processed:         15,485,535\n",
      "  Read 1 with adapter:                  56,718 (0.4%)\n",
      "  Read 2 with adapter:                  72,333 (0.5%)\n",
      "\n",
      "== Read fate breakdown ==\n",
      "Pairs that were too short:                   0 (0.0%)\n",
      "Pairs with too many N:                 138,517 (0.9%)\n",
      "Pairs written (passing filters):    15,347,018 (99.1%)\n",
      "\n",
      "Total basepairs processed: 4,645,660,500 bp\n",
      "  Read 1: 2,322,830,250 bp\n",
      "  Read 2: 2,322,830,250 bp\n",
      "Total written (filtered):  4,603,240,037 bp (99.1%)\n",
      "  Read 1: 2,301,647,966 bp\n",
      "  Read 2: 2,301,592,071 bp\n",
      "\n",
      "=== First read: Adapter 1 ===\n",
      "\n",
      "Sequence: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT; Type: regular 3'; Length: 33; Trimmed: 56718 times\n",
      "\n",
      "Minimum overlap: 3\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1; 20-29 bp: 2; 30-33 bp: 3\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 12.3%\n",
      "  C: 23.0%\n",
      "  G: 38.3%\n",
      "  T: 26.4%\n",
      "  none/other: 0.0%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t12825\t241961.5\t0\t12825\n",
      "4\t7653\t60490.4\t0\t7653\n",
      "5\t5367\t15122.6\t0\t5367\n",
      "6\t2973\t3780.6\t0\t2973\n",
      "7\t2293\t945.2\t0\t2293\n",
      "8\t3788\t236.3\t0\t3788\n",
      "9\t4168\t59.1\t0\t4160 8\n",
      "10\t3020\t14.8\t1\t2947 73\n",
      "11\t4997\t3.7\t1\t4841 156\n",
      "12\t3302\t0.9\t1\t3212 90\n",
      "13\t2620\t0.2\t1\t2537 83\n",
      "14\t3712\t0.1\t1\t0 3712\n",
      "\n",
      "\n",
      "=== Second read: Adapter 2 ===\n",
      "\n",
      "Sequence: AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC; Type: regular 3'; Length: 34; Trimmed: 72333 times\n",
      "\n",
      "Minimum overlap: 3\n",
      "No. of allowed errors:\n",
      "1-9 bp: 0; 10-19 bp: 1; 20-29 bp: 2; 30-34 bp: 3\n",
      "\n",
      "Bases preceding removed adapters:\n",
      "  A: 15.5%\n",
      "  C: 19.7%\n",
      "  G: 31.4%\n",
      "  T: 33.4%\n",
      "  none/other: 0.0%\n",
      "\n",
      "Overview of removed sequences\n",
      "length\tcount\texpect\tmax.err\terror counts\n",
      "3\t21620\t241961.5\t0\t21620\n",
      "4\t11261\t60490.4\t0\t11261\n",
      "5\t8554\t15122.6\t0\t8554\n",
      "6\t3004\t3780.6\t0\t3004\n",
      "7\t2285\t945.2\t0\t2285\n",
      "8\t3824\t236.3\t0\t3824\n",
      "9\t4223\t59.1\t0\t4146 77\n",
      "10\t3028\t14.8\t1\t2906 122\n",
      "11\t5025\t3.7\t1\t4826 199\n",
      "12\t3310\t0.9\t1\t3159 151\n",
      "13\t2551\t0.2\t1\t2456 95\n",
      "14\t3627\t0.1\t1\t1 3626\n",
      "15\t20\t0.0\t1\t0 20\n",
      "16\t1\t0.0\t1\t0 1\n",
      "原始序列对: 15485535\n",
      "保留序列对: 15347018 (99.11%)\n",
      "\n",
      "✅ 保留率符合质控标准(>90%)\n",
      "application/gzip\n",
      "application/gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started analysis of F.fq.gz\n",
      "Started analysis of R.fq.gz\n",
      "Approx 5% complete for F.fq.gz\n",
      "Approx 5% complete for R.fq.gz\n",
      "Approx 10% complete for F.fq.gz\n",
      "Approx 10% complete for R.fq.gz\n",
      "Approx 15% complete for F.fq.gz\n",
      "Approx 15% complete for R.fq.gz\n",
      "Approx 20% complete for F.fq.gz\n",
      "Approx 20% complete for R.fq.gz\n",
      "Approx 25% complete for F.fq.gz\n",
      "Approx 25% complete for R.fq.gz\n",
      "Approx 30% complete for F.fq.gz\n",
      "Approx 30% complete for R.fq.gz\n",
      "Approx 35% complete for F.fq.gz\n",
      "Approx 35% complete for R.fq.gz\n",
      "Approx 40% complete for F.fq.gz\n",
      "Approx 40% complete for R.fq.gz\n",
      "Approx 45% complete for F.fq.gz\n",
      "Approx 45% complete for R.fq.gz\n",
      "Approx 50% complete for F.fq.gz\n",
      "Approx 50% complete for R.fq.gz\n",
      "Approx 55% complete for F.fq.gz\n",
      "Approx 55% complete for R.fq.gz\n",
      "Approx 60% complete for F.fq.gz\n",
      "Approx 60% complete for R.fq.gz\n",
      "Approx 65% complete for F.fq.gz\n",
      "Approx 65% complete for R.fq.gz\n",
      "Approx 70% complete for F.fq.gz\n",
      "Approx 70% complete for R.fq.gz\n",
      "Approx 75% complete for F.fq.gz\n",
      "Approx 75% complete for R.fq.gz\n",
      "Approx 80% complete for F.fq.gz\n",
      "Approx 80% complete for R.fq.gz\n",
      "Approx 85% complete for F.fq.gz\n",
      "Approx 85% complete for R.fq.gz\n",
      "Approx 90% complete for F.fq.gz\n",
      "Approx 90% complete for R.fq.gz\n",
      "Approx 95% complete for F.fq.gz\n",
      "Approx 95% complete for R.fq.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete for F.fq.gz\n",
      "Analysis complete for R.fq.gz\n"
     ]
    }
   ],
   "source": [
    "strict_QC(origin_F, origin_R, file, primer_f=\"\", primer_r=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cf26b36-31a6-4ff1-b757-76509f5e1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd44b86-7d26-45ad-9689-4eb975810013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vsearch v2.30.0_linux_x86_64, 503.5GB RAM, 104 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Merging reads 100%\n",
      "  15347018  Pairs\n",
      "  15162346  Merged (98.8%)\n",
      "    184672  Not merged (1.2%)\n",
      "\n",
      "Pairs that failed merging due to various reasons:\n",
      "     90249  too few kmers found on same diagonal\n",
      "        73  multiple potential alignments\n",
      "     15948  too many differences\n",
      "     78377  alignment score too low, or score drop too high\n",
      "        25  overlap too short\n",
      "\n",
      "Statistics of all reads:\n",
      "    149.97  Mean read length\n",
      "\n",
      "Statistics of merged reads:\n",
      "    214.10  Mean fragment length\n",
      "     17.17  Standard deviation of fragment length\n",
      "      0.15  Mean expected error in forward sequences\n",
      "      0.17  Mean expected error in reverse sequences\n",
      "      0.13  Mean expected error in merged sequences\n",
      "      0.06  Mean observed errors in merged region of forward sequences\n",
      "      0.06  Mean observed errors in merged region of reverse sequences\n",
      "      0.12  Mean observed errors in merged region\n",
      "Started analysis of merged.fasta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双端合并完成 → .//merged.fasta\n",
      "null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Approx 5% complete for merged.fasta\n",
      "Approx 10% complete for merged.fasta\n",
      "Approx 15% complete for merged.fasta\n",
      "Approx 20% complete for merged.fasta\n",
      "Approx 25% complete for merged.fasta\n",
      "Approx 30% complete for merged.fasta\n",
      "Approx 35% complete for merged.fasta\n",
      "Approx 40% complete for merged.fasta\n",
      "Approx 45% complete for merged.fasta\n",
      "Approx 50% complete for merged.fasta\n",
      "Approx 55% complete for merged.fasta\n",
      "Approx 60% complete for merged.fasta\n",
      "Approx 65% complete for merged.fasta\n",
      "Approx 70% complete for merged.fasta\n",
      "Approx 75% complete for merged.fasta\n",
      "Approx 80% complete for merged.fasta\n",
      "Approx 85% complete for merged.fasta\n",
      "Approx 90% complete for merged.fasta\n",
      "Approx 95% complete for merged.fasta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete for merged.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TrimmomaticSE: Started with arguments:\n",
      " -threads 14 -phred33 .//merged.fasta .//output_trimmed.fastq LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15\n",
      "Input Reads: 15162346 Surviving: 15160897 (99.99%) Dropped: 1449 (0.01%)\n",
      "TrimmomaticSE: Completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "详细报告已保存至: ./file_validation_report.txt\n",
      "开始处理序列...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 15160897it [14:36, 17293.03it/s]\n",
      "36it [00:09,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "NGS(file,purpose,tempalte_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636a26fe-2bc9-4792-be49-f4397c6089c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41fe7137-8eca-4583-8d54-016120277e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 .//merged.fasta 已存在，跳过合并操作\n",
      "文件 .//output_trimmed.fastq 已存在，跳过质量控制和修剪步骤\n",
      "详细报告已保存至: ./file_validation_report.txt\n",
      "有5个条形码文件缺失，但未超过阈值 (18.0)，忽略缺失。\n",
      "输出文件未超过缺失阈值，跳过分类操作。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:09,  3.91it/s]\n"
     ]
    }
   ],
   "source": [
    "NGS(file,purpose,tempalte_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f39970b-0bb1-468a-b583-7882000870df",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "013f48f3-4f4f-47f4-b1cb-ce9a9feaaf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 .//merged.fasta 已存在，跳过合并操作\n",
      "文件 .//output_trimmed.fastq 已存在，跳过质量控制和修剪步骤\n",
      "详细报告已保存至: ./file_validation_report.txt\n",
      "有5个条形码文件缺失，但未超过阈值 (18.0)，忽略缺失。\n",
      "输出文件未超过缺失阈值，跳过分类操作。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:11,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "NGS(file,purpose,tempalte_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51180eed-9832-4198-9d2d-035075dc1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = 4\n",
    "spacer_front=20\n",
    "spacer_after=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b83ff7-279b-4a5a-8996-a0897413bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 .//merged.fasta 已存在，跳过合并操作\n",
      "文件 .//output_trimmed.fastq 已存在，跳过质量控制和修剪步骤\n",
      "详细报告已保存至: ./file_validation_report.txt\n",
      "有5个条形码文件缺失，但未超过阈值 (18.0)，忽略缺失。\n",
      "输出文件未超过缺失阈值，跳过分类操作。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spacer surrounding sequences: 36it [00:11,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "NGS(file,purpose,tempalte_CSV,spacer_front,spacer_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
